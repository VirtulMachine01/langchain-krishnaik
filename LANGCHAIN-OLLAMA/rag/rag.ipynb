{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content=\"Introduction to Large Language Models\\nLarge language models (LLMs) are a groundbreaking development in the field of artificial intelligence (AI) that have transformed the way computers process and generate human language. These advanced AI systems are trained on vast amounts of text data, enabling them to understand and mimic natural language with unprecedented accuracy and sophistication. LLMs have the ability to generate human-like text, answer questions, translate languages, and even engage in creative writing tasks. As these models continue to evolve and improve, they are poised to revolutionize various industries and applications that rely on language processing and generation.\\nHow Large Language Models Work\\nAt the core of LLMs is deep learning, a type of machine learning that utilizes artificial neural networks to identify patterns and relationships in large datasets. These neural networks are composed of multiple layers that process and transform the input data, gradually learning to recognize complex structures and semantics. LLMs are typically built using transformer models, which employ attention mechanisms to weigh the importance of different words in a sentence, allowing for better understanding of context and long-range dependencies.\\nThe training process of LLMs involves exposing the model to massive amounts of text data, such as books, articles, and websites. During this pre-training phase, the model learns to predict the next word in a sequence, gradually building its understanding of grammar, vocabulary, and world knowledge. As the model encounters more data, it becomes increasingly adept at generating coherent and contextually relevant text.\\nAfter the initial pre-training, LLMs can be fine-tuned on specific datasets or tasks, such as question answering, sentiment analysis, or domain-specific language generation. This fine-tuning process helps align the model's outputs with desired outcomes and enhances its performance in targeted areas.\\nApplications and Advantages of Large Language Models\\nThe versatility of LLMs has led to their adoption in a wide range of applications, including:\\nContent Generation: LLMs can generate human-like text for various purposes, such as articles, stories, scripts, and even poetry, making them valuable tools for writers, marketers, and content creators.\\nQuestion Answering: LLMs can provide accurate and informative answers to questions by leveraging their understanding of language and knowledge gained from training data.\\nLanguage Translation: LLMs can translate between languages with high accuracy, making them useful for international communication and collaboration.\\nSummarization: LLMs can quickly and effectively summarize lengthy documents, making it easier for users to grasp key information.\\nChatbots and Virtual Assistants: LLMs enable chatbots and virtual assistants to engage in more natural and contextually relevant conversations, improving user experience and satisfaction.\\nOne of the key advantages of LLMs is their ability to adapt to various tasks and domains. By leveraging their extensive training data and learning capabilities, LLMs can generalize patterns and perform well even with limited domain-specific data. As more data and parameters are added, LLMs continuously enhance their performance, making them valuable assets in evolving AI landscapes.\\nChallenges and Limitations of Large Language Models\\nWhile LLMs have shown remarkable progress, they also face challenges and limitations that need to be addressed. One of the primary concerns is the potential for generating inaccurate or misleading content, particularly in areas such as news and research. LLMs can sometimes produce plausible-sounding but factually incorrect information, which can be problematic if not properly vetted.\\nAnother challenge is the potential for LLMs to amplify existing biases present in the training data, leading to unfair or discriminatory outputs. Addressing these biases requires careful curation of training data and ongoing monitoring of model outputs.\\nLLMs also raise ethical concerns related to privacy, data security, and the potential for misuse. Training these models requires vast amounts of personal and sensitive data, which must be handled responsibly to protect individual privacy. Additionally, the potential for LLMs to be used for generating fake news, manipulating public opinion, or engaging in other malicious activities is a concern that needs to be addressed through robust security measures and ethical guidelines.\\nThe Future of Large Language Models\\nAs research and development in LLMs continue to advance, we can expect to see even more powerful and versatile models emerge in the coming years. Improvements in hardware, such as more efficient and powerful GPUs, will enable the training of larger and more complex models. Advancements in training techniques, such as few-shot learning and meta-learning, may allow LLMs to adapt to new tasks and domains with minimal additional training data.\\nFurthermore, the integration of LLMs with other AI technologies, such as computer vision and robotics, will likely lead to the development of multimodal systems that can understand and interact with the world in more human-like ways. These advancements could pave the way for more intelligent virtual assistants, autonomous systems, and human-AI collaboration in various domains.\\nConclusion\\nLarge language models represent a significant leap forward in the field of artificial intelligence, demonstrating the remarkable potential of deep learning and neural networks in processing and generating human language. As these models continue to evolve and find applications across various industries, it is crucial to address the challenges and limitations they present, such as the generation of inaccurate or biased content, privacy concerns, and ethical considerations.\\nBy working to mitigate these issues while harnessing the power of LLMs, we can unlock new possibilities in fields such as education, research, healthcare, and creative expression. As we move forward, it is essential to maintain a balanced and responsible approach to the development and deployment of these transformative technologies, ensuring that they serve to enhance and empower human capabilities rather than replace or endanger them.\")]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DataIngestion steps (read from pdf, web page or text files)\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(\"speech.txt\")\n",
    "\n",
    "text_documents = loader.load()\n",
    "text_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction to Large Language Models\n",
      "Large language models (LLMs) are a groundbreaking development in the field of artificial intelligence (AI) that have transformed the way computers process and generate human language. These advanced AI systems are trained on vast amounts of text data, enabling them to understand and mimic natural language with unprecedented accuracy and sophistication. LLMs have the ability to generate human-like text, answer questions, translate languages, and even engage in creative writing tasks. As these models continue to evolve and improve, they are poised to revolutionize various industries and applications that rely on language processing and generation.\n",
      "How Large Language Models Work\n",
      "At the core of LLMs is deep learning, a type of machine learning that utilizes artificial neural networks to identify patterns and relationships in large datasets. These neural networks are composed of multiple layers that process and transform the input data, gradually learning to recognize complex structures and semantics. LLMs are typically built using transformer models, which employ attention mechanisms to weigh the importance of different words in a sentence, allowing for better understanding of context and long-range dependencies.\n",
      "The training process of LLMs involves exposing the model to massive amounts of text data, such as books, articles, and websites. During this pre-training phase, the model learns to predict the next word in a sequence, gradually building its understanding of grammar, vocabulary, and world knowledge. As the model encounters more data, it becomes increasingly adept at generating coherent and contextually relevant text.\n",
      "After the initial pre-training, LLMs can be fine-tuned on specific datasets or tasks, such as question answering, sentiment analysis, or domain-specific language generation. This fine-tuning process helps align the model's outputs with desired outcomes and enhances its performance in targeted areas.\n",
      "Applications and Advantages of Large Language Models\n",
      "The versatility of LLMs has led to their adoption in a wide range of applications, including:\n",
      "Content Generation: LLMs can generate human-like text for various purposes, such as articles, stories, scripts, and even poetry, making them valuable tools for writers, marketers, and content creators.\n",
      "Question Answering: LLMs can provide accurate and informative answers to questions by leveraging their understanding of language and knowledge gained from training data.\n",
      "Language Translation: LLMs can translate between languages with high accuracy, making them useful for international communication and collaboration.\n",
      "Summarization: LLMs can quickly and effectively summarize lengthy documents, making it easier for users to grasp key information.\n",
      "Chatbots and Virtual Assistants: LLMs enable chatbots and virtual assistants to engage in more natural and contextually relevant conversations, improving user experience and satisfaction.\n",
      "One of the key advantages of LLMs is their ability to adapt to various tasks and domains. By leveraging their extensive training data and learning capabilities, LLMs can generalize patterns and perform well even with limited domain-specific data. As more data and parameters are added, LLMs continuously enhance their performance, making them valuable assets in evolving AI landscapes.\n",
      "Challenges and Limitations of Large Language Models\n",
      "While LLMs have shown remarkable progress, they also face challenges and limitations that need to be addressed. One of the primary concerns is the potential for generating inaccurate or misleading content, particularly in areas such as news and research. LLMs can sometimes produce plausible-sounding but factually incorrect information, which can be problematic if not properly vetted.\n",
      "Another challenge is the potential for LLMs to amplify existing biases present in the training data, leading to unfair or discriminatory outputs. Addressing these biases requires careful curation of training data and ongoing monitoring of model outputs.\n",
      "LLMs also raise ethical concerns related to privacy, data security, and the potential for misuse. Training these models requires vast amounts of personal and sensitive data, which must be handled responsibly to protect individual privacy. Additionally, the potential for LLMs to be used for generating fake news, manipulating public opinion, or engaging in other malicious activities is a concern that needs to be addressed through robust security measures and ethical guidelines.\n",
      "The Future of Large Language Models\n",
      "As research and development in LLMs continue to advance, we can expect to see even more powerful and versatile models emerge in the coming years. Improvements in hardware, such as more efficient and powerful GPUs, will enable the training of larger and more complex models. Advancements in training techniques, such as few-shot learning and meta-learning, may allow LLMs to adapt to new tasks and domains with minimal additional training data.\n",
      "Furthermore, the integration of LLMs with other AI technologies, such as computer vision and robotics, will likely lead to the development of multimodal systems that can understand and interact with the world in more human-like ways. These advancements could pave the way for more intelligent virtual assistants, autonomous systems, and human-AI collaboration in various domains.\n",
      "Conclusion\n",
      "Large language models represent a significant leap forward in the field of artificial intelligence, demonstrating the remarkable potential of deep learning and neural networks in processing and generating human language. As these models continue to evolve and find applications across various industries, it is crucial to address the challenges and limitations they present, such as the generation of inaccurate or biased content, privacy concerns, and ethical considerations.\n",
      "By working to mitigate these issues while harnessing the power of LLMs, we can unlock new possibilities in fields such as education, research, healthcare, and creative expression. As we move forward, it is essential to maintain a balanced and responsible approach to the development and deployment of these transformative technologies, ensuring that they serve to enhance and empower human capabilities rather than replace or endanger them.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(text_documents[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\\nThought: ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)\\n\\nFig. 2.  Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equips agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.\\n\\nFig. 3. Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\\n\\nFig. 4. Experiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)\\nChain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\\\{(x, y_i , r_i , z_i)\\\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\\\geq r_{n-1} \\\\geq \\\\dots \\\\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\\\tau_h = (x, z_i, y_i, z_j, y_j, \\\\dots, z_n, y_n)$, where $\\\\leq i \\\\leq j \\\\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.\\nTo avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\\nThe training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback and human preference dataset.\\n\\nFig. 5. After fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: Liu et al. 2023)\\nThe idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.\\n\\nFig. 6. Illustration of how Algorithm Distillation (AD) works. (Image source: Laskin et al. 2023).\\nThe paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.\\nIn reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.\\nIn comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline.\\n\\nFig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\\n\\n\\nShort-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\nFig. 8. Categorization of human memory.\\nWe can roughly consider the following mappings:\\n\\nSensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\\n\\nMaximum Inner Product Search (MIPS)#\\nThe external memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the retrieval speed, the common choice is the approximate nearest neighbors (ANN)\\u200b algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup.\\nA couple common choices of ANN algorithms for fast MIPS:\\n\\nLSH (Locality-Sensitive Hashing): It introduces a hashing function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs.\\nANNOY (Approximate Nearest Neighbors Oh Yeah): The core data structure are random projection trees, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable.\\nHNSW (Hierarchical Navigable Small World): It is inspired by the idea of small world networks where most nodes can be reached by any other nodes within a small number of steps; e.g. “six degrees of separation” feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.\\nFAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist clustering of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.\\nScaNN (Scalable Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization. It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points.\\n\\n\\nFig. 9. Comparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\\nCheck more MIPS algorithms and performance comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.\\n\\nFig. 10. A picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source: Animals using tools)\\nMRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.\\nBoth TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the “External APIs” section of Prompt Engineering.\\nChatGPT Plugins and OpenAI API  function calling are good examples of LLMs augmented with tool use capability working in practice. The collection of tool APIs can be provided by other developers (as in Plugins) or self-defined (as in function calls).\\nHuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results.\\n\\nFig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\nThe AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.\\n\\n(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\\nInstruction:\\n\\nGiven the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\n\\n(4) Response generation: LLM receives the execution results and provides summarized results to users.\\nTo put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services.\\nAPI-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.\\n\\nFig. 12. Pseudo code of how LLM makes an API call in API-Bank. (Image source: Li et al. 2023)\\nIn the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is. Decisions include:\\n\\nWhether an API call is needed.\\nIdentify the right API to call: if not good enough, LLMs need to iteratively modify the API inputs (e.g. deciding search keywords for Search Engine API).\\nResponse based on the API results: the model can choose to refine and call again if results are not satisfied.\\n\\nThis benchmark evaluates the agent’s tool use capabilities at three levels:\\n\\nLevel-1 evaluates the ability to call the API. Given an API’s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\\nLevel-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user’s requirement and learn how to use them by reading documentation.\\nLevel-3 assesses the ability to plan API beyond retrieve and call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.\\n\\nCase Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\\nIt is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.\\n\\nOne interesting observation is that while the LLM-based evaluation concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential problem with using LLM to evaluate its own performance on domains that requires deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results.\\nBoiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested to \"develop a novel anticancer drug\", the model came up with the following reasoning steps:\\n\\ninquired about current trends in anticancer drug discovery;\\nselected a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound was identified, the model attempted its synthesis.\\n\\nThey also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\nGenerative Agents Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\\n\\nMemory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X\\'s plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\nFig. 13. The generative agent architecture. (Image source: Park et al. 2023)\\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1. {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5. ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\n5. Use subprocesses for commands that will not terminate within a few minutes\\n\\nCommands:\\n1. Google Search: \"google\", args: \"input\": \"<search>\"\\n2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\\n3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\\n4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\\n5. List GPT Agents: \"list_agents\", args:\\n6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\\n7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\\n8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n9. Read file: \"read_file\", args: \"file\": \"<file>\"\\n10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\\n12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\\n13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\\n14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\\n15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\\n16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\\n17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\\n18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\\n19. Do Nothing: \"do_nothing\", args:\\n20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"\\n\\nResources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\n\\nYou should only respond in JSON format as described below\\nResponse Format:\\n{\\n    \"thoughts\": {\\n        \"text\": \"thought\",\\n        \"reasoning\": \"reasoning\",\\n        \"plan\": \"- short bulleted\\\\n- list that conveys\\\\n- long-term plan\",\\n        \"criticism\": \"constructive self-criticism\",\\n        \"speak\": \"thoughts summary to say to user\"\\n    },\\n    \"command\": {\\n        \"name\": \"command name\",\\n        \"args\": {\\n            \"arg name\": \"value\"\\n        }\\n    }\\n}\\nEnsure the response can be parsed by Python json.loads\\nGPT-Engineer is another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed.\\nHere are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\\\nThen you will pick one clarifying question, and wait for an answer from the user.\\\\n\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\\\n\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Summary of areas that need clarification:\\\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying question:\\\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next question}\\\\nIf everything is sufficiently clear, only answer \\\\\"Nothing more to clarify.\\\\\".\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Remaining unclear areas: 2 remaining questions.\\\\nCan you provide more information about how the MVC components are split into separate files?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{Make your own assumptions and state them explicitly before starting}}\"\\n  }\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:\\n\\nYou will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code’s language, and CODE is the code:\\nFILENAME\\nCODE\\nYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\nInclude module dependency or package manager dependency definition file.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\nUseful to know:\\nYou almost always put different classes in different files.\\nFor Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\nPython toolbelt preferences:\\n\\npytest\\ndataclasses\\n\\n\\nConversatin samples:\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n\"\\n  },\\n #  … same conversation as earlier, ended with \"Make your own assumptions and state them explicitly before starting\".\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Assumptions:\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\"\\n  }\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:\\n\\n\\nFinite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\\n\\nOr\\n@article{weng2023agent,\\n  title   = \"LLM-powered Autonomous Agents\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Jun\",\\n  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\\n}\\nReferences#\\n[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\\n[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\\n[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\\n“ arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\\n[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\\n[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\\n[8] Shinn & Labash. “Reflexion: an autonomous agent with dynamic memory and self-reflection” arXiv preprint arXiv:2303.11366 (2023).\\n[9] Laskin et al. “In-context Reinforcement Learning with Algorithm Distillation” ICLR 2023.\\n[10] Karpas et al. “MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.” arXiv preprint arXiv:2205.00445 (2022).\\n[11] Nakano et al. “Webgpt: Browser-assisted question-answering with human feedback.” arXiv preprint arXiv:2112.09332 (2021).\\n[12] Parisi et al. “TALM: Tool Augmented Language Models”\\n[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\\n[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.\\n[15] Li et al. “API-Bank: A Benchmark for Tool-Augmented LLMs” arXiv preprint arXiv:2304.08244 (2023).\\n[16] Shen et al. “HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace” arXiv preprint arXiv:2303.17580 (2023).\\n[17] Bran et al. “ChemCrow: Augmenting large-language models with chemistry tools.” arXiv preprint arXiv:2304.05376 (2023).\\n[18] Boiko et al. “Emergent autonomous scientific research capabilities of large language models.” arXiv preprint arXiv:2304.05332 (2023).\\n[19] Joon Sung Park, et al. “Generative Agents: Interactive Simulacra of Human Behavior.” arXiv preprint arXiv:2304.03442 (2023).\\n[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\\n[21] GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\\n')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## web based loader\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "## Load, chunk and index the content of the html page\n",
    "loader = WebBaseLoader(web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",), \n",
    "                       bs_kwargs = dict(parse_only = bs4.SoupStrainer(\n",
    "                           class_=(\"post-title\",\"post-content\",\"post-header\")\n",
    "                       )))\n",
    "\n",
    "text_documents = loader.load()\n",
    "text_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'FuseCap.pdf', 'page': 0}, page_content='FUSECAP: Leveraging Large Language Models\\nfor Enriched Fused Image Captions\\nNoam Rotstein* David Bensa ¨ıd* Shaked Brody Roy Ganz Ron Kimmel\\nTechnion - Israel Institute of Technology\\n*Indicates equal contribution.\\nAbstract\\nThe advent of vision-language pre-training techniques en-\\nhanced substantial progress in the development of models for\\nimage captioning. However, these models frequently produce\\ngeneric captions and may omit semantically important im-\\nage details. This limitation can be traced back to the image-\\ntext datasets; while their captions typically offer a general\\ndescription of image content, they frequently omit salient\\ndetails. Considering the magnitude of these datasets, man-\\nual reannotation is impractical, emphasizing the need for an\\nautomated approach. To address this challenge, we leverage\\nexisting captions and explore augmenting them with visual\\ndetails using “frozen” vision experts including an object\\ndetector, an attribute recognizer, and an Optical Character\\nRecognizer (OCR). Our proposed method, FUSECAP, fuses\\nthe outputs of such vision experts with the original captions\\nusing a large language model (LLM), yielding comprehen-\\nsive image descriptions. We automatically curate a training\\nset of 12M image-enriched caption pairs. These pairs un-\\ndergo extensive evaluation through both quantitative and\\nqualitative analyses. Subsequently, this data is utilized to\\ntrain a captioning generation BLIP-based model. This model\\noutperforms current state-of-the-art approaches, producing\\nmore precise and detailed descriptions, demonstrating the\\neffectiveness of the proposed data-centric approach. We\\nrelease this large-scale dataset of enriched image-caption\\npairs for the community.\\n1. Introduction\\nThe generation of image captions that effectively cap-\\nture essential descriptive elements has been a longstanding\\ngoal in computer vision [7, 32, 34, 36, 49, 55]. In recent\\nyears, image captioning tasks [3, 44] have gained signifi-\\ncant research attention and interest due to the success of\\nVision Language (VL) models. This achievement mainly\\nstems from the ability to efficiently harness the massive\\namount of image-caption pairs accessible online, using Vi-\\nOriginal: Two men with eye glasseslooking at somethingOurs: Two bespectacled men, one with black glasses and a black and brown beard, the other with silver glasses and short brown hair, sit together with an open blue laptop on a table in front of them. A graycat lounges nearbyOriginal: Mhmm, some clouds inthe sky Ours: A woman wearing dark sunglasses stands next to a red car with a black license plate reading 166882, PRI. The car has off and round headlights, a chrome and silver bumper, a black tire, and a red door. The cloudy and white sky is visible in the background.\\nOriginal: save yourself the expense of a professional arrangement . Ours: Floral Arrangement: A colorful assortment of sunflowers, yellow, white, orange, and purple flowers, and green leaves arranged on a black and wood table.\\nOurs: A woman with blond, long hair wearing a black belt and pants attends the premiere of The Little Stranger in 2018.Original: <PERSON> 2018 : <PERSON>: The Little Stranger Premiere -01Figure 1. FUSECAPcaptions. An illustration comparing our\\nFUSECAPenriched captions with the original ground-truth captions\\nbefore the fusing process. The examples are from COCO, SBU,\\nCC, and CC12 datasets, displayed from top to bottom.\\nsion Language Pre-training (VLP) [15, 40, 53], followed\\nby task-specific fine-tuning. However, despite remarkable\\nadvancements in image captioning, current state-of-the-art\\nmodels [14, 28, 37, 38, 68, 70, 73, 81] produce captions that\\noften overlook key semantic elements. As images are rich\\n1arXiv:2305.17718v2  [cs.CV]  15 Nov 2023'),\n",
       " Document(metadata={'source': 'FuseCap.pdf', 'page': 1}, page_content='sources of information containing intricate and complex con-\\ntent, providing precise descriptions requires highly detailed\\ntextual captions.\\nWe hypothesize that the current unsatisfactory captioning\\nresults are attributed to the image-caption datasets used for\\ntraining. Captions in these datasets frequently fail to capture\\nessential elements within images and often omit fine details.\\nFor example, consider the original caption of the top image\\nin Figure 1. The caption is missing details such as the laptop\\nand cat. Since these datasets contain a massive number of\\nimage-caption pairs, manual re-annotation is unfeasible.\\nIn this work, our primary objective is to develop a frame-\\nwork that produces richer, more accurate captions for im-\\nages. To achieve this, we introduce FUSECAP, a novel\\napproach designed to automatically augment captions in ex-\\nisting datasets, thereby enhancing training data of the model.\\nThis contrasts with methods that primarily focus on improv-\\ning the caption generator model architecture and resonates\\nwith the recently surveyed data-centric artificial intelligence\\n(AI) paradigm [79, 80], which underscores the significance\\nof improving both the quality and quantity of data, rather\\nthan merely concentrating on the advancement of model de-\\nsign. We leverage the capabilities of vision experts such as\\nobject detectors [4], attribute recognizers [82], and Optical\\nCharacter Recognition (OCR) models [1, 2, 8]. The visual\\ninformation extracted by these models is intended to provide\\ncomplementary details to the original simplistic captions. By\\nharnessing the reasoning capabilities of a dedicated LLM,\\nthe outputs from the vision experts are fused with the original\\nimage caption. This results in a coherent, meaningful nat-\\nural description of images that is more comprehensive and\\ndetailed than the original caption, as illustrated in Figure 1.\\nSpecifically, we leverage ChatGPT [11] to generate “fusing”\\nexamples which are then used to fine-tune a pre-trained Flan-\\nT5 model [54]. We apply this method to enrich captions\\nof a human-annotated dataset (COCO [44]) and large-scale\\ndatasets collected from the web (CC12 [12], CC [58], and\\nSBU [65]). This process produces an enriched collection\\nconsisting of 12M image-text pairs. To confirm the quality\\nof our generated dataset, we first show that humans favor\\nthe fused captions, perceiving them as more descriptive and\\naccurate than the original captions. Furthermore, we demon-\\nstrate that the fused captions score higher on CLIPScore [26]\\n– a reference-free metric that evaluates text-image alignment\\nwithout using reference captions — compared to the origi-\\nnal ones. To further emphasize their effectiveness, we also\\nassess the captions through image-to-text and text-to-image\\nretrieval tasks.\\nTo illustrate the benefits of the proposed data-centric ap-\\nproach we capitalize on these fused captions to train a cap-\\ntion generator. We use the augmented dataset both for pre-\\ntraining and for the fine-tuning of an image captioning BLIP\\nmodel [38]. Despite having fewer parameters and using lesstraining data, our model surpasses existing state-of-the-art\\nmethods [37, 38, 46, 69, 70] in generating comprehensive\\ncaptions. This superiority is evident both in its higher CLIP-\\nScore and its improved performance in the retrieval tasks.\\nThe performance advantage is further illustrated by numer-\\nous examples.\\nOur contributions are as follows:\\n•Introducing FUSECAP- a novel approach to automati-\\ncally enrich existing image-captions datasets by fusing\\noutputs from visual experts using an LLM.\\n•Providing a large dataset of 12M caption-enriched text-\\nimage pairs for future research.\\n•Showcasing that an enriched dataset leads to models\\ncapable of generating detailed captions that effectively\\nincorporate previously overlooked key semantic ele-\\nments.\\n2. Related Efforts\\nImage-Caption Generation. Image captioning has been a\\nwidely researched topic at the intersection of computer vision\\nand natural language processing. Early strategies for caption\\ngeneration made use of retrieval-based methods [23, 27] and\\ntemplate-based methods [35], which were limited in their\\nexpressiveness [6]. The advent of deep learning marked a\\nshift in this field, as multimodal neural networks enabled\\nthe generation of higher quality captions [31]. Subsequently,\\nthe encoder-decoder framework, which essentially translates\\nan image into a sentence, became one of the favored ap-\\nproaches [20,66]. Later advancements incorporated attention\\nmechanisms for focusing on key image aspects [76,78]. Cur-\\nrent best captioning techniques are transformer-based archi-\\ntectures [63], which combine vision [21] and language [63]\\ntransformers. The recent advancements in the effectiveness\\nof image captioning can largely be credited to the introduc-\\ntion of visual language pre-training (VLP).\\nVLP uses large-scale image-text pairs to pre-train vision-\\ntext models, which are later fine-tuned for downstream tasks,\\nwith image captioning as one of the central tasks [25, 37,38].\\nVLP can be categorized [13] into single-stream [16, 41] and\\ndual-stream architectures [22, 24, 29, 53], aiming to merge\\nvisual and textual modalities into a shared embedding space.\\nEfforts exploring integrating text generative tasks for pre-\\ntraining [17, 72] have fueled the development of models like\\nBLIP [38] and recent BLIP-2 [37]. OFA [70] proposed uni-\\nfying multiple unimodal and multimodal pre-training tasks,\\nwhich led to a significant performance improvement.\\nDense captioning can be seen as a task related to ours in\\ngenerating comprehensive text for images [30,77]. However,\\nit produces multiple captions for various regions within an\\nimage, as opposed to a single descriptive caption for the\\nwhole image. Previous approaches to generating more com-\\nprehensive captions have consistently identified a common\\n2'),\n",
       " Document(metadata={'source': 'FuseCap.pdf', 'page': 2}, page_content='Vision ExpertsOriginalImage-CaptionPair\\na woman is holding a tennis racket at a game  A woman wearing a yellow Nike shirt and black shorts holds a white and black tennis racket while playing against a blue wall with Emirates Airline branding. She accessorizes with a gold necklace and raises her hand in excitement.LLM FuserAttributes ExtractorDetector\\nOCR\\nNIKEEmiratesAirlineFusedCaption(a) Fusing Enriched Captions\\n  A woman wearing a yellow Nike shirt and black shorts holds a white and black tennis racket while playing against a blue wall with Emirates Airline branding. She accessorizes with a gold necklace and raises her hand in excitement.  E woman wearing a yellow Nike shirt and black shorts holds a white and black tennis racket while playing against a blue wall with Emirates Airline branding. She accessorizes with a gold necklace and raises her hand in excitement.Caption Generation Pretrain+Finetune\\n  A woman wearing a yellow Nike shirt and black shorts holds a white and black tennis racket while playing against a blue wall with Emirates Airline branding. She accessorizes with a gold necklace and raises her hand in excitement.  A man with a gray beard and dark sunglasses stands next to a green sign with various golf course information. He holds a yellow frisbee in his hand. A tall tree stands in the background.\\nEnrichedImage-CaptionPair\\n(b) Training a Captioning Model\\nFigure 2. Our Approach Illustration. Figure 2a illustrates the automated process of enriching existing image captions using the proposed\\nFUSECAPapproach. Visual experts extract meaningful information from images, which is then fused with the original captions by an LLM\\nFuser, producing rich captions. Following this, Figure 2b illustrates the utilization of image datasets, paired with these augmented captions,\\nin both the pre-training and fine-tuning phases of a comprehensive image-captioning model.\\nchallenge – the unsatisfactory capability of existing image\\ncaptioners of providing detailed and accurate textual descrip-\\ntions of images [45, 47, 48]. These approaches focus on\\nimproving captions discriminability, whereas our method\\nseeks to enrich captions with additional, meaningful infor-\\nmation extracted from the images.\\nImage-Caption Datasets Current datasets for image-\\ncaption pairs fall into two primary categories: specifically\\nhuman-annotated datasets like COCO [44], and web-crawled\\ndatasets such as CC, CC12, and SBU Captions [12, 50, 58].\\nThe conjunction of the datasets serves as the foundation for\\nthe VLP, subsequently followed by downstream tasks fine-\\ntuning. In general, the first category of datasets is smaller in\\nsize yet exhibits substantially lower noise levels compared\\nto the second category. Both categories, however, are char-\\nacterized by relatively short and concise captions. Specific\\nexamples of these characteristics can be seen in Figure 1.\\nData-centric AI underscores the importance of refining\\ndata quality to boost model performance rather than fine-\\ntuning model designs. It prioritizes curating, labeling, and\\ncleaning data for superior training datasets [61] as well as\\nautomating its processing [79, 80]. This new emphasis sug-\\ngests that with high-quality data, even basic algorithms candeliver impressive results. [59] identified the need to enrich\\nimage-text datasets. To this end, they harnessed natural lan-\\nguage inference (NLI) to fuse multiple existing ground-truth\\ncaptions into a single one. However, this approach can oper-\\nate only on datasets with multiple ground-truth captions and\\ncannot be applied to large-scale ones with a single caption\\n(e.g., CC, CC12, and SBU). In contrast, our method can be\\napplied to any image-caption dataset.\\nLarge Language Models Large language models (LLMs)\\nhave been shown to be effective in a wide range of tasks,\\nincluding natural language inference, question answering,\\nand code generation [18,19,74]. Further, LLMs such as GPT-\\n3 [11] exhibit impressive zero and few-shot performance on\\na variety of tasks, including translation, text summarization,\\nand common sense reasoning, without further fine-tuning.\\nFew-shot abilities allow researchers to use LLMs as a tool\\nfor data generation. [10] used GPT-3 to generate instructions\\nand edited captions dataset, which is then used to train model\\nfor image editing. [57] presented an approach for training\\nLLMs to use external API calls. For example, when the\\nLLM is been asked to solve a mathematical problem, it\\ncan use an API of a calculator, instead of generating the\\noutput by itself. [57] used GPT-3 few-shot ability to curate\\n3'),\n",
       " Document(metadata={'source': 'FuseCap.pdf', 'page': 3}, page_content='a dataset of external API calls which is then been used to\\nfine-tune another LLM. [52] used GPT-4 outputs to create\\nan instruction-following dataset that can be used later to fine-\\ntune other LLMs in a supervised learning fashion. To fuse\\ntogether the original caption and the visual expert outputs, in\\nthis work, we harness the impressive zero-shot capabilities of\\nOpenAI’s ChatGPT [11]. We employ it to generate a small\\n“fusing” dataset. To establish an open-source framework that\\nscales cost-effectively, we fine-tune Flan-T5 [19], a widely\\nrecognized open-source LLM, using this data.\\nVision Experts in VLP Several works attempted to im-\\nprove VLP by incorporating object detectors or other ex-\\nperts [46] as part of their initialization [17, 41], architec-\\nture [46], pre-training data [67] or pre-training objectives\\n[42, 60, 62, 82]. In image captioning, such models demon-\\nstrate limited capabilities in generating rich captions and\\nhave not fully capitalized on the information provided by\\nvision experts. We hypothesize that this limitation stems\\ndirectly from the succinct captions in existing image-text\\ndatasets. In our data-centric approach, we focus on im-\\nproving such datasets and overcoming this limitation. An\\nexample that highlights our approach is the contrast with\\nPrismer [46]. Unlike our method, Prismer augments its\\nmodel architecture with object detectors and OCR for cap-\\ntion generation but relies on traditional captioning datasets.\\nA recent concurrent paper [71] proposed an LLM-based\\nmodel that merges captions of image segments into text. In\\ncontrast, we adopt the LLM from a data-centric perspective\\nto enhance existing caption datasets, an approach that is fol-\\nlowed by extensive evaluations for quality and consistency.\\n3. Fusing Enriched Captions\\nIn this section, we introduce FUSECAP, which is illus-\\ntrated in Figure 2a. This novel strategy is designed to au-\\ntomatically augment existing image captions by integrating\\nimportant details and inter-object relations within the image.\\nThese details are often disregarded in traditional image cap-\\ntioning datasets. First, we elaborate on our use of pre-trained\\nvision models, referred to as vision experts , for extracting\\nrelevant visual information from images. We then detail how\\nthe information gathered from these expert models is subse-\\nquently fused with the original caption through a fine-tuned\\nLLM, resulting in the enhanced captions.\\n3.1. Vision Expert Models\\nTo enrich the information found in the original caption,\\nwe employ the following vision experts:\\nObject Detection A key visual expert we rely on is an\\nobject detector model. Following the approach proposed\\nin [82], we utilize a Faster-RCNN [56] with a ResNeXt152\\n[75] backbone. This model is initially pre-trained on sev-\\neral detection datasets and then fine-tuned on the VisualGenome (VG) dataset [33]. The data used comprises more\\nthan 100K images with 1.6K classes, enabling strong gen-\\neralization abilities. We regard all objects along with their\\ncorresponding bounding boxes as valid detections, provided\\nthey exceed a detection confidence threshold. The presence\\nand position of objects in the scene provide essential details\\nto complement the caption.\\nAttributes Prediction Beyond class identification, we de-\\nrive a variety of attributes for each object in the image using\\nfeatures generated by our Faster-RCNN within each bound-\\ning box. A classification model proposed in [82] was trained\\non annotations from the VG dataset, which covers a broad\\narray of 400 distinct attributes. These attributes encapsulate\\nvarious aspects of the objects, including size, condition, and\\ncolor. For each object, we only consider attributes predicted\\nwith a confidence level above the threshold.\\nText Detection and Recognition Text within images often\\ncontains critical contextual information, leading to a line\\nof different text-image tasks [9, 25]. To incorporate textual\\ninformation, we utilize robust pre-trained OCR models to\\ndetect and extract characters. We first identify text within\\na scene with CRAFT [5], a robust scene-text detector. We\\nthen apply Parseq [8], a state-of-the-art scene text recognizer,\\nto decode the text within the bounding boxes generated by\\nthe text detector. To avoid contamination, we do not apply\\nOCR methods on datasets presenting watermarks, such as\\nthe CC [58] and CC12 [12].\\n3.2. LLM Fuser\\nTo generate natural and coherent captions, which are es-\\nsential for the caption generation task, we utilize a large lan-\\nguage model (LLM). We leverage a specifically fine-tuned\\nLLM to fuse insights from various vision experts into the\\noriginal caption, creating a single coherent and fluent de-\\nscription. The LLM’s advanced reasoning capabilities allow\\nus to articulate the semantic relationships among objects and\\nto seamlessly integrate the diverse knowledge provided by\\nthese experts. Consequently, the output captures the essence\\nof the visual content while the impressive generative capacity\\nof LLMs ensures these captions remain coherent and natural.\\nTo train the LLM, we create a small “fusing” dataset with\\nChatGPT [11], and then use it to fine-tune the open-source\\nFlanT5-XL model [19].\\nChatGPT Annotation We leverage the zero-shot capa-\\nbilities of ChatGPT to generate a “fusing” dataset of 20K\\nexamples, which is then used to fine-tune an open-source\\nLLM. By employing this approach, we establish a framework\\nbased on open-source models that can generate a large-scale\\ndataset at a reasonable cost. To produce enriched captions\\nwith ChatGPT, we first extract information from the visual\\n4'),\n",
       " Document(metadata={'source': 'FuseCap.pdf', 'page': 4}, page_content='OFA:a surfer in a wetsuit rides a wave.GIT: woman in a wetsuit is surfing on a waveBLIP2: a woman in a wet suit riding a wave on a surfboardOurs: a barefoot surfer with long brown hair rides a white wave on a white surfboard, extending their arm and hand for balance\\nPrismer: A woman riding a wave on top of a surfboard\\nOFA: motorcycle parked on the beachGIT: a red motorcycle parked on a road near a beachBLIP2: a red motorcycle parked in a parking lot next to a fenceOurs: a red motorcycle with a leather and black seat is parked on the side of the road, surrounded by a wood fence and tall palm trees the clear blue sky provides a serene backdropPrismer: A red motorcycle parked on the side of a road.BLIP2: a boat is in the water and birds are on the shore Ours: a serene sunset scene with a small boat anchored in calm waters, surrounded by a flock of birds and a dark cloud in the sky\\nOFA: a flock of birds are swimming in the water at sunsetGIT: a large body of water with a boat in the distance.Prismer:  A large body of water filled with birds under a cloudy sky.\\nOurs: a snowboarder glides down a snow -covered mountain under a gray sky, wearing tan and khaki pants and casting a dark shadow\\nBLIP2: a group of people on snowboards on a snowy slopOFA: a man riding a snowboard down a snow coveredslopeGIT: a group of people riding snowboards on top of a snow coveredslope.Prismer: A group of people on snowboards and skis in the snow.Figure 3. Image Captioning Results. While top-performing captioning models tend to provide concise and oversimplified captions, our\\nmodel outputs rich captions that better describe the images.\\nexperts presented in Section 3.1. The objects and their cor-\\nresponding attributes and detected texts are then ordered\\nfrom left to right based on their bounding boxes, providing\\nbasic spatial context. The exact prompt can be found in the\\nappendix. We generated 20K examples of such enriched\\ncaptions from CC, CC12, SBU, and COCO datasets, which\\nserved as training data for the FlanT5-XL model.\\nLLM Fine-tuning Using the “fusing” dataset we created,\\nwe fine-tuned Flan-T5 [19], a variant of the T5 encoder-\\ndecoder model [54], that has been extensively fine-tuned\\non numerous tasks to achieve exceptional performance on\\ninstruction-based tasks. Specifically, we utilized the Flan-T5-\\nXL checkpoint and fine-tuned it to our curated fuse dataset.\\nDuring the fine-tuning process, the original caption was con-\\ncatenated with the output of the visual experts and served as\\nthe model input, while the enriched caption was designated\\nas the target. The hyperparameters we used for this section\\ncan be found in the appendix. After this phase, the fine-tuned\\nmodel can be used to generate an enriched captions dataset.\\n4. Training a Captioning Model\\nWe apply FUSECAP, presented in Section 3, to the COCO,\\nSBU, CC, and CC12 datasets, yielding 12 million aug-\\nmented image-caption pairs. Following our data-centric\\napproach, and to demonstrate the effectiveness of this aug-\\nmented dataset, we use it to train a captioning model based\\non the BLIP architecture [38] (Figure 2b). Adopting the\\ntraining strategy from the original BLIP paper, we first per-\\nform vision-language pre-training for 20epochs , optimizing\\nthree objectives: (a) Image-Text Contrastive Loss (ITC)that aligns visual and textual features by contrasting between\\nmatching and non-matching image-text pairs, (b) Image-\\nText Matching Loss (ITM) that classifies image-text pairs\\nas matched or unmatched, and (c) Generative Language\\nModeling Loss (LM) that generates textual captions from\\nimages, which are compared to their ground truth captions.\\nFollowing the pre-training phase, adhering to common prac-\\ntice in VLP [37,38,81], we fine-tune our model for a supple-\\nmentary 5epochs on the enriched COCO dataset, utilizing\\nsolely LM loss. To allow the generation of more comprehen-\\nsive captions, we increase the context length over that used\\nin the original BLIP model from 30to60tokens. The hyper-\\nparameters for this section are detailed in the appendix.\\n5. Experiments\\nWe evaluate two sets of image captions. The first set\\nconsists of the fused captions from the FUSECAPdataset,\\ndescribed in Section 3. The second set includes captions\\ngenerated by the trained captioning model discussed in Sec-\\ntion 4. The objective for both sets is to produce captions\\nthat are descriptive and accurate. Traditionally, image cap-\\ntioning methods are assessed with n-gram-based metrics like\\nBLEU [51], CIDEr [64], and ROUGE [43]. These metrics\\ncompare the tested captions to reference captions, assum-\\ning the latter represents the ideal image descriptions that\\nthe tested captions aim for. However, as highlighted earlier,\\ncaptions in existing datasets often fail to provide a compre-\\nhensive description of images. This implies that given our\\nemphasis on enhancing descriptiveness, relying on broad\\nand non-specific reference captions for evaluation would be\\ninappropriate. Consequently, n-gram-based metrics do not\\n5'),\n",
       " Document(metadata={'source': 'FuseCap.pdf', 'page': 5}, page_content='effectively measure or promote this quality [84].\\nWe, therefore adopt CLIPScore [26], a reference-free met-\\nric that measures the alignment between textual and visual\\nembeddings generated by a pre-trained CLIP model [53].\\nSince CLIPScore is not dependent on a reference caption, its\\nscore is not restricted by the descriptiveness of the original\\ncaptions. Moreover, in terms of accuracy, it has been shown\\nto exhibit a higher correlation with human judgments than\\nn-gram-based metrics [26].\\nBesides evaluating enriched captions with CLIPScore, we\\nconsider the ability to perform image-text retrieval as a perti-\\nnent metric for caption evaluation. Comprehensive captions\\nshould inherently serve as distinct image descriptors, thereby\\nimproving retrieval precision. To further assess the quality\\nof captions in the FUSECAPdataset, we conducted a human\\nevaluation study.\\n5.1. FuseCap Dataset\\nIn this subsection, we evaluate the enriched captions pro-\\nduced by the caption augmentation approach introduced\\nin the Section 3. In particular, we carry out a qualitative\\nhuman-study alongside quantitative evaluations. Examples\\nof enriched captions via FuseCap are showcased in Figure 1.\\nQualitative Evaluation We conducted a thorough human\\nevaluation study to assess the ability of our enriched captions\\nto be both descriptive and relevant to the images. Specifically,\\nwe randomly sampled 400 pairs from the COCO dataset and\\nprovided 40 participants with (1) the original caption and\\n(2) the enriched caption. Our study engaged a pool of ran-\\ndom internet users as participants. To minimize biases and\\nensure an impartial evaluation, they completed the survey\\nunaware of the specific research objectives or goals. The\\nevaluators were asked the following: “Does caption 2 pro-\\nvide an additional meaningful and truthful description of\\nthe image compared to caption 1?” . As the enriched cap-\\ntions are much more detailed, as can be seen in Figure 1, the\\nDataset Captions Mean V oting\\nCOCOOriginal 76.7 31.7%\\nFUSECAP 80.3 67.6%\\nSBUOriginal 71.9 32.1%\\nFUSECAP 75.5 60.2%\\nCCOriginal 72.6 34.7%\\nFUSECAP 75.4 59.7%\\nTable 1. FUSECAPdata quantitative evaluation. CLIPScore-\\nbased comparison between the original captions of common image-\\ntext datasets with our enriched ones. “Mean” indicated the mean\\nCLIPScore and “V oting” expresses CLIP’s preference in a one-\\nvs-one setting. As can be seen, FUSECAPobtains significantly\\nimproved results in both metrics.question focuses on whether they are accurate. Our study\\nresults indicate that participants find the enriched captions\\nat least as good as the original ones in 72.9%of the images.\\nThis finding highlights the proposed method’s effectiveness\\nin enhancing the captions’ descriptiveness while preserving\\nalignment and relevancy to the images.\\nQuantitative Evaluation We evaluate the enriched cap-\\ntions in comparison to the original captions for each dataset\\nunder consideration. To this end, we randomly selected 5000\\nimages from each dataset and report the CLIPScore obtained\\nwith both types of captions. As depicted in Table 1, the\\nenriched captions generated by our proposed method con-\\nsistently achieve a higher CLIPScore (Mean) on average by\\n4.6%. In addition, given an image and two captions (original\\nand enriched), we utilize CLIP to measure which caption\\nis preferred. We evaluate this on the different datasets and\\nsummarize it under “V oting”, which our captions outperform\\ncurrent ones on average by 29.7%. These results demon-\\nstrate the effectiveness of our approach in generating en-\\nriched captions that better reflect the content of the images.\\nImage-Text Retrieval To further demonstrate the descrip-\\ntiveness and accuracy of the FUSECAPdata, we assess its\\nperformance in the image-text retrieval task. This task in-\\nvolves matching images to text queries and vice versa. If\\nthe enriched data is descriptive and accurate, retrieval per-\\nformance should improve since the additional details can\\nserve as a discriminative factor in establishing these cor-\\nrespondences. Our training methodology aligns with the\\noriginal BLIP model. After pre-training the model on a\\nlarge-scale dataset as in Section 4, the model is fine-tuned\\nfor image-text retrieval on the COCO training set using both\\nITC and ITM losses. For the inference step, we employ the\\nmethod proposed by [39], previously integrated into BLIP.\\nThis method involves the selection of Kcandidates based\\non feature similarity, followed by their re-ranking using re-\\nspective ITM values. We report R@Nthat corresponds to\\nthe accuracy of retrieving the true text/image among the top\\nNretrieved results. The details of the competing models\\nare provided in Section 5.2, with the exception that the ver-\\nsions discussed here have been fine-tuned for the retrieval\\ntasks. As illustrated in Table 2, the use of fused captions\\ncontributes significantly to the enhancement of retrieval per-\\nformance on the COCO test set1. For example, compared to\\nthe model trained on corresponding non-enriched data, the\\nR@1score for image-to-text retrieval increased by 22.1%,\\nand for text-to-image retrieval, it increased by 34.8%.\\n5.2. Caption Generation\\nWe fine-tune the BLIP model that was pre-trained on\\nthe complete FUSECAPdataset for captioning, using the\\n1While we used enriched captions for training and testing BLIP FUSECAP,\\nfor the other baselines we used the original dataset without enrichment.\\n6'),\n",
       " Document(metadata={'source': 'FuseCap.pdf', 'page': 6}, page_content='COCO Retrieval\\nimg→text text →img\\nModel R@1 R@5 R@10 R@1 R@5 R@10\\nBLIP† 75.1 92.7 96.4 58.2 82.4 89.2\\nBLIP-L 82.4 95.4 97.9 65.2 86.3 91.8\\nBLIP2 85.4 97.0 98.5 68.3 87.7 92.6\\nBLIP∗\\nFUSECAP 97.2 99.5 99.9 93.0 97.4 98.3\\nTable 2. Image-text retrieval results. Performance on COCO\\nretrieval (test sets). The “*” symbol indicates that the model was\\ntrained and tested on our enriched dataset. These results attest that\\ngiven rich captions, BLIP FUSECAPsignificantly outperforms existing\\nmethods, which utilize standard captions.\\nenriched COCO dataset. We refer to this fine-tuned version\\nas BLIP FUSECAP. To assess the effectiveness of the captioner,\\nand, by extension, the FUSECAPdata it is trained on, we\\nthoroughly compare our results to various state-of-the-art\\ncaptioning models using CLIPScore. In particular, we con-\\nsider the following baselines:\\n•BLIP†[38]: An original BLIP model, pre-trained and\\nfine-tuned on the same image set as BLIP FUSECAP, which\\nuses original captions, in contrast to BLIP FUSECAP\\nwhich uses the enriched captions. To guarantee a fair\\ncomparison, we set configuration parameters identical\\nto those implemented in BLIP FUSECAP.\\n•BLIP-L : A large version of BLIP, which was pre-\\ntrained on a dataset comprising 129M images prior\\nto the captioning fine-tuning. Among all models pre-\\nsented in the original BLIP paper, this model achieves\\nsuperior captioning results.\\n•BLIP2-G-OPT 2.7[37]: A state-of-the-art captioning\\nmodel that integrates a large frozen vision backbone\\n(ViT G) along with a frozen LLM (OPT [83]), trained\\non the datasets considered for BLIP-L.\\n•OFA [70]: A state-of-the-art image captioning method\\ntrained on unimodal and multimodal pre-training tasks\\nand fine-tuned for captioning.\\n•GIT [69]: A state-of-the-art vision-language model,\\nwith an image encoder and text decoder architecture.\\nIt is scaled up in terms of both pre-training data and\\nmodel size, and it is fine-tuned for image captioning.\\n•Prismer [46]: A Vision-Language Model with Multi-\\nModal Experts such as an object detector and an OCR\\nmodel. Prismer adopts a strategy that could be seen as\\ndual to ours, integrating vision experts into the model\\narchitecture, as opposed to leveraging them to enrich\\nthe training data.Model Images Parameters Val Test\\nBLIP† 12M 247M 75.2 75.3\\nBLIP-L 129M 470M 76.1 76.0\\nOFA 20M 470M 76.6 76.4\\nGIT 800M 700M 77.1 77.0\\nBLIP2-G-OPT 2.7 129M 3.8B 77.8 77.5\\nPrismer 13M 1.6B 76.7 76.7\\nBLIP FUSECAP 12M 247M 78.3 78.5\\nTable 3. Image captioning results. CLIPScore of leading models\\nand our approach on the COCO captions dataset. Our model outper-\\nforms much larger models that have pre-trained with significantly\\nmore training data.\\nBuilding on the methodology outlined in Section 5, we\\nevaluate the performance of our models using the mean\\nCLIPScore metric. As shown in Table 3, our model not only\\noutperforms the BLIP †model by 4.3%but also surpasses the\\nbest performing among other models by 1.3%. This improve-\\nment over models with a considerably larger parameter count\\nthat have been trained on significantly more image-caption\\npairs underscores the effectiveness of our data centric ap-\\nproach. Furthermore, as illustrated in Figure 3, our approach\\nexhibits clear superiority in the generation of captions, pro-\\nducing descriptions with greater semantic detail compared\\nto those generated by competing models. Our results sur-\\npassing Prismer indicate that for generating comprehensive\\nimage captions, leveraging vision experts to enrich data can\\nbe more beneficial than incorporating them directly into the\\nmodel architecture. Additional examples showcasing the\\nperformance and capabilities of our model can be found in\\nthe supplementary material.\\nImage-Text Retrieval To further assess the quality of\\nthe generated captions, we once again utilize the image-\\ntext retrieval task, following the method detailed in Sec-\\ntion 5.1. This time, however, we consider captions not\\nfrom the ground truth COCO or the “ground truth” enriched\\nCOCO datasets. Instead, they are produced by the BLIP †\\nand BLIP FUSECAPcaption generators. Captions from BLIP †\\nand BLIP FUSECAPare assessed using retrieval models fine-\\ntuned on original and enriched captions, respectively, both of\\nthese retrieval models are the ones discussed in Section 5.1.\\nTable 4 shows that by using the captions from the captioners\\ninstead of the ground truth captions, BLIP FUSECAPretains\\nits superiority over BLIP †in both image-to-text and text-\\nto-image retrievals. Impressively, using the BLIP FUSECAP\\ncaptions, the retrieval performance is on par with results\\nachieved using the ground truth enriched captions in Table 2.\\nIn contrast, BLIP †exhibits a significant performance drop\\nwhen compared to its results with the corresponding ground\\ntruth captions.\\n7'),\n",
       " Document(metadata={'source': 'FuseCap.pdf', 'page': 7}, page_content='COCO Retrieval\\nimg→text text →img\\nModel R@1 R@5 R@10 R@1 R@5 R@10\\nBLIP†56.3 83.0 90.3 54.5 81.2 88.7\\n-18.8% -9.7% -6.1% -3.7% -1.2% -0.5%\\nBLIP∗\\nFUSECAP95.0 98.8 99.2 94.5 98.7 99.3\\n-2.2% -0.7% -0.7% +1.5% +1.3% +1%\\nTable 4. Image-text retrieval results using generated captions.\\nPerformance on COCO retrieval (test sets) using captions generated\\nby the models presented in Section 4 rather than ground truth\\ncaptions. The models used for evaluation are the ones referenced in\\nSection 5.1. The highlighted percentages indicate the performance\\ndifference when compared to the use of corresponding ground truth\\ncaptions in Table 2. Notably, BLIP FUSECAPcaptions excel over\\nBLIP †, a trend consistent with ground truth captions. Moreover,\\nBLIP FUSECAPmaintains a performance parity with the ground truth\\ndatasets, while BLIP †shows a noticeable decline in performance.\\n5.3. Large-Scale Data Influence\\nTypical VLP frameworks involve pre-training vision-\\nlanguage models on image-caption pairs before fine-tuning\\nthem on downstream tasks. Given this structure, one might\\nargue that the generation of enriched captions could serve\\nas an additional downstream task. This would suggest using\\nthe standard captions dataset for pre-training, followed by\\nfine-tuning on a smaller enriched dataset. Yet, as mentioned\\nin Section 5.2, our approach with the BLIP FUSECAPmodel\\ndeviates from this by pre-training on enriched datasets in-\\nstead of the conventional ones. In this section, we evaluate\\nthe impact of this deviation.\\nWe compare the performance of models that are pre-\\ntrained on both the standard and enriched datasets. Both\\nmodels are subsequently fine-tuned using the enriched\\nCOCO dataset. Unlike the evaluation method detailed in\\nSection 5, which is reference-free, this section employs con-\\nventional, reference-based evaluation metrics, as both mod-\\nels are fine-tuned and tested using the same dataset. Table 5\\nreveals that the model pre-trained on the enriched captions\\noutperforms its counterpart, which was pre-trained on the\\noriginal captions, across all metrics when both are fine-tuned\\non the augmented COCO dataset. This outcome underscores\\nthe importance of using large-scale enriched datasets for\\npre-training and validates our automated dataset creation\\napproach. Interestingly, the model that is pre-trained on the\\nenriched captions and fine-tuned using the standard COCO\\ndataset performs comparably to the model pre-trained on the\\noriginal captions and fine-tuned on the same dataset.\\n6. Limitations\\nIn the human evaluation study (Section 5.1), a subset of\\nparticipants favored the original caption. This may be due to\\nour fusing process occasionally missing inter-element depen-Pre-training\\nDataFine-tune\\n+Test DataB@4 CIDEr SPICE\\nStandard Standard 37.8 126.5 22.9\\nFUSECAP Standard 38.4 128.7 23.0\\nStandard FUSECAP 35.4 111.4 25.0\\nFUSECAP FUSECAP 37.3 123.1 26.8\\nTable 5. Influence of Large-Scale Data. Pre-training with the\\nFUSECAPdataset significantly outperforms pre-training on stan-\\ndard data when fine-tuned for enriched caption generation. This\\nunderscores the value of the large-scale FUSECAPdataset and its\\nautomated creation approach. Additionally, when fine-tuned for\\nstandard captioning, the FUSECAPpre-trained model is comparable\\nto, if not slightly better than, the standard data pre-trained model.\\ndencies, as seen in Figure 1, where the cat’s position between\\nthe man and the laptop was not captured. Future research\\nmight integrate finer visual details, like segmentation, and\\nrefine the LLM fuser accordingly.\\nEthical considerations FUSECAPdataset was constructed\\nusing FlanT5-XL model [19], which was trained on unfil-\\ntered data potentially laden with explicit content or inherent\\nbiases. Consequently, the proposed dataset may replicate\\nbiases from the original model.\\n7. Conclusions\\nIn this paper, we address the problem of generating highly\\ndescriptive and detailed image captions. We observe that\\nexisting state-of-the-art methods produce short and often\\noversimplified captions that fail to capture the intricate de-\\ntails in images. We hypothesize that this is due to datasets\\nlimitation, i.e.,existing training data composed of concise\\ncaptions. Thus, the tendency to provide such captions is\\ndistilled into the trained models, regardless of their architec-\\nture or training method. Therefore, we introduce a novel and\\ngeneric data-centric automated strategy to enrich existing im-\\nage captions, termed FUSECAP. Specifically, this approach\\nharnesses visual experts to extract meaningful information\\nfrom images and an LLM to fuse such data into the existing\\ncaptions, yielding enriched ones. We apply FUSECAPto dif-\\nferent widespread datasets and generate 12M image-enriched\\ncaption pairs. The augmented image captions quality is eval-\\nuated qualitatively using a human evaluation survey, and\\nquantitatively, using different evaluation methods. Finally,\\nwe demonstrate the effectiveness of the enriched data by\\nutilizing it to train an image captioning model, which outper-\\nforms significantly larger state-of-the-art methods. We posit\\nthat our research highlights the marked potential of LLMs\\nin enhancing powerful data-centric approaches in computer\\nvision.\\n8'),\n",
       " Document(metadata={'source': 'FuseCap.pdf', 'page': 8}, page_content='References\\n[1]Aviad Aberdam, David Bensa ¨ıd, Alona Golts, Roy Ganz,\\nOren Nuriel, Royee Tichauer, Shai Mazor, and Ron Litman.\\nClipter: Looking at the bigger picture in scene text recogni-\\ntion. arXiv preprint arXiv:2301.07464 , 2023. 2\\n[2]Aviad Aberdam, Roy Ganz, Shai Mazor, and Ron Litman.\\nMultimodal semi-supervised learning for text recognition.\\narXiv preprint arXiv:2205.03873 , 2022. 2\\n[3]Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen,\\nRishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Ste-\\nfan Lee, and Peter Anderson. Nocaps: Novel object caption-\\ning at scale. In Proceedings of the IEEE/CVF International\\nConference on Computer Vision , pages 8948–8957, 2019. 1\\n[4]Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney,\\nMark Johnson, Stephen Gould, and Lei Zhang. Bottom-up\\nand top-down attention for image captioning and visual ques-\\ntion answering. In Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition , pages 6077–6086,\\n2018. 2\\n[5]Youngmin Baek, Bado Lee, Dongyoon Han, Sangdoo Yun,\\nand Hwalsuk Lee. Character region awareness for text detec-\\ntion. In Proceedings of the IEEE/CVF conference on com-\\nputer vision and pattern recognition , pages 9365–9374, 2019.\\n4, 13\\n[6]Shuang Bai and Shan An. A survey on automatic image\\ncaption generation. Neurocomputing , 311:291–304, 2018. 2\\n[7]K. Barnard and D. Forsyth. Learning the semantics of words\\nand pictures. In Proceedings Eighth IEEE International Con-\\nference on Computer Vision. ICCV 2001 , volume 2, pages\\n408–415 vol.2, 2001. 1\\n[8]Darwin Bautista and Rowel Atienza. Scene text recognition\\nwith permuted autoregressive sequence models. In European\\nConference on Computer Vision , pages 178–196, Cham, 10\\n2022. Springer Nature Switzerland. 2, 4, 13\\n[9]Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez,\\nMarc ¸al Rusinol, Ernest Valveny, CV Jawahar, and Dimos-\\nthenis Karatzas. Scene text visual question answering. In\\nProceedings of the IEEE/CVF international conference on\\ncomputer vision , pages 4291–4301, 2019. 4\\n[10] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\\nstructpix2pix: Learning to follow image editing instructions.\\narXiv preprint arXiv:2211.09800 , 2022. 3\\n[11] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\\nPranav Shyam, Girish Sastry, Amanda Askell, et al. Language\\nmodels are few-shot learners. Advances in neural information\\nprocessing systems , 33:1877–1901, 2020. 2, 3, 4\\n[12] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu\\nSoricut. Conceptual 12m: Pushing web-scale image-text pre-\\ntraining to recognize long-tail visual concepts. In Proceedings\\nof the IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition , pages 3558–3568, 2021. 2, 3, 4\\n[13] Fei-Long Chen, Du-Zhen Zhang, Ming-Lun Han, Xiu-Yi\\nChen, Jing Shi, Shuang Xu, and Bo Xu. Vlp: A survey on\\nvision-language pre-training. Machine Intelligence Research ,\\n20(1):38–56, 2023. 2[14] Xi Chen, Xiao Wang, Soravit Changpinyo, A. J. Piergiovanni,\\nPiotr Padlewski, Daniel Salz, Sebastian Goodman, Adam\\nGrycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov,\\nJoan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari,\\nGaurav Mishra, Linting Xue, Ashish Thapliyal, James Brad-\\nbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia,\\nBurcu Karagol Ayan, Carlos Riquelme, Andreas Steiner,\\nAnelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu\\nSoricut. Pali: A jointly-scaled multilingual language-image\\nmodel. CoRR , 2022. 1\\n[15] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy,\\nFaisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu.\\nUNITER: universal image-text representation learning. In An-\\ndrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael\\nFrahm, editors, Computer Vision - ECCV 2020 - 16th Euro-\\npean Conference, Glasgow, UK, August 23-28, 2020, Pro-\\nceedings, Part XXX . Springer, 2020. 1\\n[16] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy,\\nFaisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter:\\nUniversal image-text representation learning. In Computer\\nVision–ECCV 2020: 16th European Conference, Glasgow,\\nUK, August 23–28, 2020, Proceedings, Part XXX , pages 104–\\n120. Springer, 2020. 2\\n[17] Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unifying\\nvision-and-language tasks via text generation. In Interna-\\ntional Conference on Machine Learning , pages 1931–1942.\\nPMLR, 2021. 2, 4\\n[18] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham,\\nHyung Won Chung, Charles Sutton, Sebastian Gehrmann,\\net al. Palm: Scaling language modeling with pathways. arXiv\\npreprint arXiv:2204.02311 , 2022. 3\\n[19] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph,\\nYi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa\\nDehghani, Siddhartha Brahma, et al. Scaling instruction-\\nfinetuned language models. arXiv preprint arXiv:2210.11416 ,\\n2022. 3, 4, 5, 8, 13\\n[20] Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama,\\nMarcus Rohrbach, Subhashini Venugopalan, Kate Saenko,\\nand Trevor Darrell. Long-term recurrent convolutional net-\\nworks for visual recognition and description. In Proceedings\\nof the IEEE conference on computer vision and pattern recog-\\nnition , pages 2625–2634, 2015. 2\\n[21] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\\nvain Gelly, et al. An image is worth 16x16 words: Trans-\\nformers for image recognition at scale. arXiv preprint\\narXiv:2010.11929 , 2020. 2\\n[22] Zi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuohang\\nWang, Lijuan Wang, Chenguang Zhu, Pengchuan Zhang, Lu\\nYuan, Nanyun Peng, et al. An empirical study of training\\nend-to-end vision-and-language transformers. In Proceedings\\nof the IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition , pages 18166–18176, 2022. 2\\n[23] Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi, Pe-\\nter Young, Cyrus Rashtchian, Julia Hockenmaier, and David\\nForsyth. Every picture tells a story: Generating sentences\\n9'),\n",
       " Document(metadata={'source': 'FuseCap.pdf', 'page': 9}, page_content='from images. In Computer Vision–ECCV 2010: 11th Eu-\\nropean Conference on Computer Vision, Heraklion, Crete,\\nGreece, September 5-11, 2010, Proceedings, Part IV 11 , pages\\n15–29. Springer, 2010. 2\\n[24] Roy Ganz and Michael Elad. Clipag: Towards generator-free\\ntext-to-image generation. arXiv preprint arXiv:2306.16805 ,\\n2023. 2\\n[25] Roy Ganz, Oren Nuriel, Aviad Aberdam, Yair Kittenplon,\\nShai Mazor, and Ron Litman. Towards models that can see\\nand read. arXiv preprint arXiv:2301.07389 , 2023. 2, 4\\n[26] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,\\nand Yejin Choi. Clipscore: A reference-free evaluation metric\\nfor image captioning, 2022. 2, 6\\n[27] Micah Hodosh, Peter Young, and Julia Hockenmaier. Fram-\\ning image description as a ranking task: Data, models and\\nevaluation metrics. Journal of Artificial Intelligence Research ,\\n47:853–899, 2013. 2\\n[28] Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang,\\nZicheng Liu, Yumao Lu, and Lijuan Wang. Scaling up vision-\\nlanguage pre-training for image captioning. In Proceedings\\nof the IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition , pages 17980–17989, 2022. 1\\n[29] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,\\nHieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom\\nDuerig. Scaling up visual and vision-language representation\\nlearning with noisy text supervision. In International Confer-\\nence on Machine Learning , pages 4904–4916. PMLR, 2021.\\n2\\n[30] Justin Johnson, Andrej Karpathy, and Li Fei-Fei. Densecap:\\nFully convolutional localization networks for dense caption-\\ning. In Proceedings of the IEEE conference on computer\\nvision and pattern recognition , pages 4565–4574, 2016. 2\\n[31] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic align-\\nments for generating image descriptions. In Proceedings of\\nthe IEEE conference on computer vision and pattern recogni-\\ntion, pages 3128–3137, 2015. 2\\n[32] Andrej Karpathy, Armand Joulin, and Li Fei-Fei. Deep frag-\\nment embeddings for bidirectional image sentence mapping.\\nInAdvances in Neural Information Processing Systems 27:\\nAnnual Conference on Neural Information Processing Sys-\\ntems 2014, December 8-13 2014, Montreal, Quebec, Canada ,\\n2014. 1\\n[33] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,\\nKenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-\\ntidis, Li-Jia Li, David A Shamma, et al. Visual genome:\\nConnecting language and vision using crowdsourced dense\\nimage annotations. International journal of computer vision ,\\n123:32–73, 2017. 4\\n[34] Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming Li,\\nYejin Choi, Alexander C. Berg, and Tamara L. Berg. Baby\\ntalk: Understanding and generating simple image descriptions.\\nInThe 24th IEEE Conference on Computer Vision and Pattern\\nRecognition, CVPR 2011, Colorado Springs, CO, USA, 20-25\\nJune 2011 , 2011. 1\\n[35] Girish Kulkarni, Visruth Premraj, Vicente Ordonez, Sagnik\\nDhar, Siming Li, Yejin Choi, Alexander C Berg, and Tamara L\\nBerg. Babytalk: Understanding and generating simple im-age descriptions. IEEE transactions on pattern analysis and\\nmachine intelligence , 35(12):2891–2903, 2013. 2\\n[36] Polina Kuznetsova, Vicente Ordonez, Alexander C. Berg,\\nTamara L. Berg, and Yejin Choi. Collective generation of\\nnatural image descriptions. In The 50th Annual Meeting of the\\nAssociation for Computational Linguistics, Proceedings of\\nthe Conference, July 8-14, 2012, Jeju Island, Korea - Volume\\n1: Long Papers , 2012. 1\\n[37] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-\\n2: Bootstrapping language-image pre-training with frozen\\nimage encoders and large language models. arXiv preprint\\narXiv:2301.12597 , 2023. 1, 2, 5, 7\\n[38] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip:\\nBootstrapping language-image pre-training for unified vision-\\nlanguage understanding and generation. In International Con-\\nference on Machine Learning , pages 12888–12900. PMLR,\\n2022. 1, 2, 5, 7, 13\\n[39] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq\\nJoty, Caiming Xiong, and Steven Chu Hong Hoi. Align\\nbefore fuse: Vision and language representation learning\\nwith momentum distillation. Advances in neural information\\nprocessing systems , 34:9694–9705, 2021. 6\\n[40] Junnan Li, Ramprasaath R. Selvaraju, Akhilesh Gotmare,\\nShafiq R. Joty, Caiming Xiong, and Steven Chu-Hong Hoi.\\nAlign before fuse: Vision and language representation learn-\\ning with momentum distillation. In Marc’Aurelio Ranzato,\\nAlina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jen-\\nnifer Wortman Vaughan, editors, Advances in Neural Infor-\\nmation Processing Systems 34: Annual Conference on Neural\\nInformation Processing Systems 2021, NeurIPS 2021, Decem-\\nber 6-14, 2021, virtual , 2021. 1\\n[41] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh,\\nand Kai-Wei Chang. Visualbert: A simple and perfor-\\nmant baseline for vision and language. arXiv preprint\\narXiv:1908.03557 , 2019. 2, 4\\n[42] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei\\nHu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu\\nWei, et al. Oscar: Object-semantics aligned pre-training for\\nvision-language tasks. In Computer Vision–ECCV 2020: 16th\\nEuropean Conference, Glasgow, UK, August 23–28, 2020,\\nProceedings, Part XXX 16 , pages 121–137. Springer, 2020. 4\\n[43] Chin-Yew Lin. Rouge: A package for automatic evaluation\\nof summaries. In Text summarization branches out , pages\\n74–81, 2004. 5\\n[44] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\\nPietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence\\nZitnick. Microsoft coco: Common objects in context. In\\nComputer Vision–ECCV 2014: 13th European Conference,\\nZurich, Switzerland, September 6-12, 2014, Proceedings, Part\\nV 13, pages 740–755. Springer, 2014. 1, 2, 3\\n[45] Lixin Liu, Jiajun Tang, Xiaojun Wan, and Zongming Guo.\\nGenerating diverse and descriptive image captions using vi-\\nsual paraphrases. In Proceedings of the IEEE/CVF Interna-\\ntional Conference on Computer Vision , pages 4240–4249,\\n2019. 3\\n[46] Shikun Liu, Linxi Fan, Edward Johns, Zhiding Yu, Chaowei\\nXiao, and Anima Anandkumar. Prismer: A vision-language\\n10'),\n",
       " Document(metadata={'source': 'FuseCap.pdf', 'page': 10}, page_content='model with an ensemble of experts. arXiv preprint\\narXiv:2303.02506 , 2023. 2, 4, 7\\n[47] Xihui Liu, Hongsheng Li, Jing Shao, Dapeng Chen, and Xi-\\naogang Wang. Show, tell and discriminate: Image captioning\\nby self-retrieval with partially labeled data. In Proceedings of\\nthe European conference on computer vision (ECCV) , pages\\n338–354, 2018. 3\\n[48] Ruotian Luo, Brian Price, Scott Cohen, and Gregory\\nShakhnarovich. Discriminability objective for training de-\\nscriptive captions. In Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition , pages 6964–6974,\\n2018. 3\\n[49] Rebecca Mason and Eugene Charniak. Nonparametric\\nmethod for data-driven image captioning. In Proceedings\\nof the 52nd Annual Meeting of the Association for Computa-\\ntional Linguistics, ACL 2014, June 22-27, 2014, Baltimore,\\nMD, USA, Volume 2: Short Papers , 2014. 1\\n[50] Vicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text:\\nDescribing images using 1 million captioned photographs.\\nAdvances in neural information processing systems , 24, 2011.\\n3\\n[51] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing\\nZhu. Bleu: a method for automatic evaluation of machine\\ntranslation. In Proceedings of the 40th annual meeting of the\\nAssociation for Computational Linguistics , pages 311–318,\\n2002. 5\\n[52] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley,\\nand Jianfeng Gao. Instruction tuning with gpt-4. arXiv\\npreprint arXiv:2304.03277 , 2023. 4\\n[53] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\\ntransferable visual models from natural language supervi-\\nsion. In International conference on machine learning , pages\\n8748–8763. PMLR, 2021. 1, 2, 6\\n[54] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\\nSharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\\nPeter J Liu. Exploring the limits of transfer learning with\\na unified text-to-text transformer. The Journal of Machine\\nLearning Research , 21(1):5485–5551, 2020. 2, 5\\n[55] Krishnan Ramnath, Simon Baker, Lucy Vanderwende, Mo-\\ntaz Ahmad El-Saban, Sudipta N. Sinha, Anitha Kannan, No-\\nran Hassan, Michel Galley, Yi Yang, Deva Ramanan, Alessan-\\ndro Bergamo, and Lorenzo Torresani. Autocaption: Auto-\\nmatic caption generation for personal photos. In IEEE Winter\\nConference on Applications of Computer Vision, Steamboat\\nSprings, CO, USA, March 24-26, 2014 , 2014. 1\\n[56] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\\nFaster r-cnn: Towards real-time object detection with region\\nproposal networks. Advances in neural information process-\\ning systems , 28, 2015. 4\\n[57] Timo Schick, Jane Dwivedi-Yu, Roberto Dess `ı, Roberta\\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Can-\\ncedda, and Thomas Scialom. Toolformer: Language mod-\\nels can teach themselves to use tools. arXiv preprint\\narXiv:2302.04761 , 2023. 3\\n[58] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu\\nSoricut. Conceptual captions: A cleaned, hypernymed, im-age alt-text dataset for automatic image captioning. In Pro-\\nceedings of the 56th Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers) , pages\\n2556–2565, 2018. 2, 3, 4\\n[59] Zhan Shi, Hui Liu, and Xiaodan Zhu. Enhancing descrip-\\ntive image captioning with natural language inference. In\\nProceedings of the 59th Annual Meeting of the Association\\nfor Computational Linguistics and the 11th International\\nJoint Conference on Natural Language Processing (Volume\\n2: Short Papers) , pages 269–277, 2021. 3\\n[60] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu\\nWei, and Jifeng Dai. Vl-bert: Pre-training of generic visual-\\nlinguistic representations. arXiv preprint arXiv:1908.08530 ,\\n2019. 4\\n[61] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhi-\\nnav Gupta. Revisiting unreasonable effectiveness of data in\\ndeep learning era. In Proceedings of the IEEE international\\nconference on computer vision , pages 843–852, 2017. 3\\n[62] Hao Tan and Mohit Bansal. Lxmert: Learning cross-modality\\nencoder representations from transformers. arXiv preprint\\narXiv:1908.07490 , 2019. 4\\n[63] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\\nPolosukhin. Attention is all you need. Advances in neural\\ninformation processing systems , 30, 2017. 2\\n[64] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi\\nParikh. Cider: Consensus-based image description evalu-\\nation. In Proceedings of the IEEE conference on computer\\nvision and pattern recognition , pages 4566–4575, 2015. 5\\n[65] Tom ´as F Yago Vicente, Le Hou, Chen-Ping Yu, Minh Hoai,\\nand Dimitris Samaras. Large-scale training of shadow detec-\\ntors with noisily-annotated shadow examples. In Computer\\nVision–ECCV 2016: 14th European Conference, Amsterdam,\\nThe Netherlands, October 11-14, 2016, Proceedings, Part VI\\n14, pages 816–832. Springer, 2016. 2\\n[66] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru\\nErhan. Show and tell: A neural image caption generator. In\\nProceedings of the IEEE conference on computer vision and\\npattern recognition , pages 3156–3164, 2015. 2\\n[67] Alex Jinpeng Wang, Pan Zhou, Mike Zheng Shou, and\\nShuicheng Yan. Position-guided text prompt for vision-\\nlanguage pre-training. arXiv preprint arXiv:2212.09737 ,\\n2022. 4\\n[68] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li,\\nKevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang.\\nGit: A generative image-to-text transformer for vision and\\nlanguage. arXiv preprint arXiv:2205.14100 , 2022. 1\\n[69] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li,\\nKevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang.\\nGIT: A generative image-to-text transformer for vision and\\nlanguage. CoRR , 2022. 2, 7\\n[70] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai,\\nZhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and\\nHongxia Yang. Ofa: Unifying architectures, tasks, and modal-\\nities through a simple sequence-to-sequence learning frame-\\nwork, 2022. 1, 2, 7\\n[71] Teng Wang, Jinrui Zhang, Junjie Fei, Yixiao Ge, Hao\\nZheng, Yunlong Tang, Zhe Li, Mingqi Gao, Shanshan Zhao,\\n11'),\n",
       " Document(metadata={'source': 'FuseCap.pdf', 'page': 11}, page_content='Ying Shan, et al. Caption anything: Interactive image de-\\nscription with diverse multimodal controls. arXiv preprint\\narXiv:2305.02677 , 2023. 4\\n[72] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia\\nTsvetkov, and Yuan Cao. Simvlm: Simple visual language\\nmodel pretraining with weak supervision. arXiv preprint\\narXiv:2108.10904 , 2021. 2\\n[73] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia\\nTsvetkov, and Yuan Cao. Simvlm: Simple visual language\\nmodel pretraining with weak supervision. In The Tenth In-\\nternational Conference on Learning Representations, ICLR\\n2022, Virtual Event, April 25-29, 2022 , 2022. 1\\n[74] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu,\\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and\\nQuoc V Le. Finetuned language models are zero-shot learners.\\narXiv preprint arXiv:2109.01652 , 2021. 3\\n[75] Saining Xie, Ross Girshick, Piotr Doll ´ar, Zhuowen Tu, and\\nKaiming He. Aggregated residual transformations for deep\\nneural networks. In Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition , pages 1492–1500,\\n2017. 4\\n[76] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron\\nCourville, Ruslan Salakhudinov, Rich Zemel, and Yoshua\\nBengio. Show, attend and tell: Neural image caption gener-\\nation with visual attention. In International conference on\\nmachine learning , pages 2048–2057. PMLR, 2015. 2\\n[77] Linjie Yang, Kevin Tang, Jianchao Yang, and Li-Jia Li. Dense\\ncaptioning with joint inference and visual context. In Proceed-\\nings of the IEEE conference on computer vision and pattern\\nrecognition , pages 2193–2202, 2017. 2\\n[78] Zhilin Yang, Ye Yuan, Yuexin Wu, William W Cohen, and\\nRuss R Salakhutdinov. Review networks for caption genera-\\ntion. Advances in neural information processing systems , 29,\\n2016. 2\\n[79] Daochen Zha, Zaid Pervaiz Bhat, Kwei-Herng Lai, Fan Yang,\\nand Xia Hu. Data-centric ai: Perspectives and challenges. In\\nProceedings of the 2023 SIAM International Conference on\\nData Mining (SDM) , pages 945–948. SIAM, 2023. 2, 3\\n[80] Daochen Zha, Zaid Pervaiz Bhat, Kwei-Herng Lai, Fan\\nYang, Zhimeng Jiang, Shaochen Zhong, and Xia Hu. Data-\\ncentric artificial intelligence: A survey. arXiv preprint\\narXiv:2303.10158 , 2023. 2, 3\\n[81] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei\\nZhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. Vinvl:\\nRevisiting visual representations in vision-language models.\\nInIEEE Conference on Computer Vision and Pattern Recog-\\nnition, CVPR 2021, virtual, June 19-25, 2021 , 2021. 1, 5\\n[82] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei\\nZhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. Vinvl:\\nRevisiting visual representations in vision-language models.\\nInProceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition , pages 5579–5588, 2021. 2, 4\\n[83] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,\\nMoya Chen, Shuohui Chen, Christopher Dewan, Mona Diab,\\nXian Li, Xi Victoria Lin, et al. Opt: Open pre-trained trans-\\nformer language models. arXiv preprint arXiv:2205.01068 ,\\n2022. 7[84] Deyao Zhu, Jun Chen, Kilichbek Haydarov, Xiaoqian Shen,\\nWenxuan Zhang, and Mohamed Elhoseiny. Chatgpt asks, blip-\\n2 answers: Automatic questioning towards enriched visual\\ndescriptions. arXiv preprint arXiv:2303.06594 , 2023. 6\\n12'),\n",
       " Document(metadata={'source': 'FuseCap.pdf', 'page': 12}, page_content='A. F USECAPFused Captions\\nIn this section, we present supplementary details about\\nthe newly proposed fused dataset presented in Section 3. Ap-\\npendix A.2 presents examples of this enriched data, adding\\nto the ones shown in Figure 1.\\nA.1. Visual Experts Implementation\\nWe utilize the Visual Experts discussed in Section 3.1, in\\nthe following manner,\\n•Object detection We consider objects as valid detec-\\ntions if they surpass a pre-determined detection confi-\\ndence threshold of 0.7.\\n•Attribute Detection We incorporate attributes to each\\nvalid predicted object if the attribute confidence sur-\\npasses a 0.2 threshold.\\n•OCR : We use CRAFT and Parseq with default infer-\\nence parameters [5,8]. The text recognized is attributed\\nto the object that has the smallest bounding box encom-\\npassing it.\\nA.2. Training Set Generation for LLM Fuser with\\nChatGPT.\\nAs outlined in Section 3.2, we harness the zero-shot ca-\\npabilities of ChatGPT to generate a compact dataset encom-\\npassing 20,000 examples. This data is subsequently used\\nto fine-tune an open-source Large Language Model (LLM).\\nThe prompt provided to ChatGPT is as follows:\\n”A caption of an image is given: original caption .\\nThe following objects are detected in the image from left to\\nright:\\nAa1\\n1, ...,ak−1\\n1andak\\n1o1[with the following text: t1].\\n...\\nAa1\\nn, ...,akn−1\\nn andaknnon[with the following text: tn].\\nWrite a comprehensive and concise caption of the scene\\nusing the objects detected.”\\nOriginal: A man preparing to catch a frisbee in front of some houses.Ours: A man in white and blue shorts prepares to catch a white frisbee in front of a stone wall and a black metal fence, with a brown and red roof in the background, under a blue sky.Original: The Bug Club | Membership | Romney Hythe & Dymchurch RailwayOurs: A black and orange train, belonging to The Bug Club, sits on the tracks of Romney Hythe & Dymchurch Railway.\\nWe denote {oi}N\\ni=1as the set of objects detected, {aj\\ni}N\\ni=1\\nas the attributes related to each object, and {ti}N\\ni=1as texts\\nrelated to each object.\\nB. Training Settings\\nB.1. LLM Fuser\\nOur LLM Fuser is a fine-tuned FlanT5-XL [19] model.\\nWe used the huggingface library to fine-tune it on a sin-\\ngle NVIDIA A40 GPU. We trained the model for 4,000\\noptimization steps, with batch size of 32, a learning rate of\\n5·10−5, and a linear scheduling strategy. We limit the source\\nand target length to 100 and 200 tokens, respectively.\\nB.2. Caption Generator\\nFigure 5 offers further examples, supplementing those\\nfound in Figure 3 and providing additional instances of the\\nmodel’s outputs. These examples further emphasize the\\nability of the captioning model to generate semantically rich\\ncaptions.\\nTraining. The pre-training and subsequent fine-tuning of\\nthe captioning model, described in Section 5, was performed\\non eight NVIDIA A100 GPUs. For setting the pre-training\\nhyperparameters, we followed the approach outlined in the\\noriginal BLIP implementation [38], except for maximum\\ncaption length, batch size, and initial learning rate. We devi-\\nated from the original model’s batch size to accommodate\\nthe increased token length used in our implementation. To\\nmaintain stability during pre-training with a smaller batch\\nsize, we reduced the initial learning rate. The batch size used\\nfor pre-training was 400(50per GPU). The initial learning\\nrate used was 6·10−5.\\nMaximum Caption Length. The original BLIP model\\nimposes a training and inference limit of 30tokens per cap-\\ntion. With our enrichment process, however, caption lengths\\ntend to be longer and beyond this original threshold. Ac-\\ncordingly, we have increased the maximum caption length\\nto 60 tokens. This limit is maintained through both the pre-\\ntraining and fine-tuning stages of the caption generator, as\\nwell as during the actual generation of captions.\\n13'),\n",
       " Document(metadata={'source': 'FuseCap.pdf', 'page': 13}, page_content='Original: A hard black case sits outside a door.Ours: A hard black case sits outside a wood door with the text \"DESPARPAJO\" and a pink handle.\\nOriginal: rainbow lorikeet bird in treeOurs: A rainbow lorikeet bird perches on a tree amidst green leaves and blue sky, with its striking red body, blue head, and vibrant blue and orange beak.\\nOriginal: Hard times teach us valuable lessons. Handwriting on a napkin with a cup of coffee stock photosOurs: A cup of coffee sits on a red saucer, accompanied by a white napkin and a metal pen. The scene suggests a reminder that hard times teach valuable lessons, as captured in handwritten text.Original: happy kids sitting on the grass.Ours: A group of happy blond girls sitting on the grass, wearing red, yellow, and blue shirts, with one girl having a small nose and another having a closed eye.\\nOriginal: A man playing tennis in a tennis court Ours: A man in a white shirt and red shorts plays tennis on a court with a blue racket and white socks. He wears dark sunglasses and stands in front of a chain link fence and a green tree.\\nOriginal: A sign in my bed room.Ours: A black letter hangs on a white wall with the text \"Shack\" in my bedroom.Figure 4. Examples of the proposed F USECAPenriched-captions dataset.\\nBLIP2:a man and woman sitting on a motorcycle outsideOurs: a smiling man and woman pose for a picture in front of a red motorcycle, with the man wearing a blue shirt and black glasses, and the woman wearing a white shirt\\nOFA: a man and a woman sitting on a motorcycleGIT: a man and a woman posing for a picture.Prismer:A man and a woman sitting on a motorcycle.\\nOurs: a white cow rests in a lush green field under a clear blue sky, with a large green tree in the background and a dark shadow cast on the groundBLIP2: a cow is laying in the grass on a sunny dayOFA: a white cow sitting in a grassy fieldGIT: a cow laying in a field with a tree in the background.Prismer: A white cow laying on a lush green field.\\n14'),\n",
       " Document(metadata={'source': 'FuseCap.pdf', 'page': 14}, page_content='Ours: a man wearing a white shirt and blue jeans rides a motorbike in a parking lot surrounded by white and yellow tents, with a white line marking the edge of the parking\\nBLIP2: a man riding a motorcycle in a parking lot with tentsOFA: a man riding a motorcycle in a parking lotGIT: a man riding a small motorcycle in a parking lot.Prismer:A man riding a motorcycle in a parking lot.\\nOurs: a man wearing a red hat and blue jeans rides a white horse with a long tail, while a small white dog follows closely behind\\nBLIP2:a man riding a horse with a dog in the fieldOFA: a man riding on the back of a white horseGIT: a man riding a horse with a dog in the background.Prismer: A man riding a horse next to a small dog.\\nOurs: a woman with brown hair wearing a white shirt and black pants throws a white frisbee into a metal basket on a green grassy field\\nBLIP2: a woman is holding a frisbee in front of a metal cageOFA: a woman throwing a frisbee into a metal basketGIT: a woman is holding a frisbee in her hand.Prismer: woman throwing a frisbee into a metal cage.\\nBLIP2: a large elephant is riding in the back of a truckOurs: a brown elephant stands on the back of a truck on a gray road surrounded by green trees and grass under a cloudy sky, with a white line marking the edge of the roadOFA: an elephant is riding in the back of a truckGIT: an elephant is riding in the back of a truck.Prismer:An elephant is riding in the back of a truck.\\nOurs: a black bird soars over a lush green valley, with a brown building in the foreground and a white and blue sky overheadBLIP2: a bird flying over a green valley with mountains in the backgroundOFA: a bird flies over the alleghenymountainsGIT: a bird flying over a city with mountains in the background.Prismer: A bird flying over a lush green hillside.15'),\n",
       " Document(metadata={'source': 'FuseCap.pdf', 'page': 15}, page_content='Ours: a military helicopter with black blades and propellers flies through a gray sky, emitting orange and red smoke\\nBLIP2: a helicopter is flying through the airwith smoke coming out of the tailOFA: a helicopter flying in the skyGIT: a helicopter is flying through the airwith smoke coming out of it.Prismer: A helicopter flying through a gray sky with propellers.\\nOurs: a colorful assortment of vegetables, including carrots, celery, and lettuce, are arranged on a counter next to a white blender with a green handle\\nBLIP2:a pile of vegetables sitting on a counter next to a juicerOFA: a pile of vegetables sitting next to a white blenderGIT: a bunch of carrots and other vegetables on a counter.Prismer: A close upof a blender and vegetables on a table.\\nOFA: a woman holding an umbrella in front of a shopGIT: a woman holding an umbrella outside a shop.\\nOurs: a woman stands in front of a store, holding a white umbrella she wears blue jeans, a white scarf, and blue boots in the background, there is a wicker basketPrismer: A woman holding a white umbrella over her head.\\nBLIP2:a woman holding an umbrella outside a store on a cobblestone streetFigure 5. Comparative illustration of captions generated by our BLIP FUSECAPand other top-performing models.\\n16')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## PDF loader\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"FuseCap.pdf\")\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'FuseCap.pdf', 'page': 0}, page_content='FUSECAP: Leveraging Large Language Models\\nfor Enriched Fused Image Captions\\nNoam Rotstein* David Bensa ¨ıd* Shaked Brody Roy Ganz Ron Kimmel\\nTechnion - Israel Institute of Technology\\n*Indicates equal contribution.\\nAbstract\\nThe advent of vision-language pre-training techniques en-\\nhanced substantial progress in the development of models for\\nimage captioning. However, these models frequently produce\\ngeneric captions and may omit semantically important im-\\nage details. This limitation can be traced back to the image-\\ntext datasets; while their captions typically offer a general\\ndescription of image content, they frequently omit salient\\ndetails. Considering the magnitude of these datasets, man-\\nual reannotation is impractical, emphasizing the need for an\\nautomated approach. To address this challenge, we leverage\\nexisting captions and explore augmenting them with visual\\ndetails using “frozen” vision experts including an object\\ndetector, an attribute recognizer, and an Optical Character'),\n",
       " Document(metadata={'source': 'FuseCap.pdf', 'page': 0}, page_content='existing captions and explore augmenting them with visual\\ndetails using “frozen” vision experts including an object\\ndetector, an attribute recognizer, and an Optical Character\\nRecognizer (OCR). Our proposed method, FUSECAP, fuses\\nthe outputs of such vision experts with the original captions\\nusing a large language model (LLM), yielding comprehen-\\nsive image descriptions. We automatically curate a training\\nset of 12M image-enriched caption pairs. These pairs un-\\ndergo extensive evaluation through both quantitative and\\nqualitative analyses. Subsequently, this data is utilized to\\ntrain a captioning generation BLIP-based model. This model\\noutperforms current state-of-the-art approaches, producing\\nmore precise and detailed descriptions, demonstrating the\\neffectiveness of the proposed data-centric approach. We\\nrelease this large-scale dataset of enriched image-caption\\npairs for the community.\\n1. Introduction\\nThe generation of image captions that effectively cap-'),\n",
       " Document(metadata={'source': 'FuseCap.pdf', 'page': 0}, page_content='release this large-scale dataset of enriched image-caption\\npairs for the community.\\n1. Introduction\\nThe generation of image captions that effectively cap-\\nture essential descriptive elements has been a longstanding\\ngoal in computer vision [7, 32, 34, 36, 49, 55]. In recent\\nyears, image captioning tasks [3, 44] have gained signifi-\\ncant research attention and interest due to the success of\\nVision Language (VL) models. This achievement mainly\\nstems from the ability to efficiently harness the massive\\namount of image-caption pairs accessible online, using Vi-'),\n",
       " Document(metadata={'source': 'FuseCap.pdf', 'page': 0}, page_content='Vision Language (VL) models. This achievement mainly\\nstems from the ability to efficiently harness the massive\\namount of image-caption pairs accessible online, using Vi-\\nOriginal: Two men with eye glasseslooking at somethingOurs: Two bespectacled men, one with black glasses and a black and brown beard, the other with silver glasses and short brown hair, sit together with an open blue laptop on a table in front of them. A graycat lounges nearbyOriginal: Mhmm, some clouds inthe sky Ours: A woman wearing dark sunglasses stands next to a red car with a black license plate reading 166882, PRI. The car has off and round headlights, a chrome and silver bumper, a black tire, and a red door. The cloudy and white sky is visible in the background.\\nOriginal: save yourself the expense of a professional arrangement . Ours: Floral Arrangement: A colorful assortment of sunflowers, yellow, white, orange, and purple flowers, and green leaves arranged on a black and wood table.'),\n",
       " Document(metadata={'source': 'FuseCap.pdf', 'page': 0}, page_content='Ours: A woman with blond, long hair wearing a black belt and pants attends the premiere of The Little Stranger in 2018.Original: <PERSON> 2018 : <PERSON>: The Little Stranger Premiere -01Figure 1. FUSECAPcaptions. An illustration comparing our\\nFUSECAPenriched captions with the original ground-truth captions\\nbefore the fusing process. The examples are from COCO, SBU,\\nCC, and CC12 datasets, displayed from top to bottom.\\nsion Language Pre-training (VLP) [15, 40, 53], followed\\nby task-specific fine-tuning. However, despite remarkable\\nadvancements in image captioning, current state-of-the-art\\nmodels [14, 28, 37, 38, 68, 70, 73, 81] produce captions that\\noften overlook key semantic elements. As images are rich\\n1arXiv:2305.17718v2  [cs.CV]  15 Nov 2023')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Making the chunks from the loaded data\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "documents[:5] # 5 chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents) # total 93 chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Embeddings\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "embeddings = OllamaEmbeddings(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'chromadb' has no attribute 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 9\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Chroma\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# persist_directory = 'chroma-db'\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# vectordb = Chroma.from_documents(documents, embeddings)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Save the vector store\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# vectordb.persist()\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m vector_store \u001b[38;5;241m=\u001b[39m \u001b[43mChroma\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./chroma_langchain_db\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Where to save data locally, remove if not neccesary\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:205\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     emit_warning()\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain_community/vectorstores/chroma.py:117\u001b[0m, in \u001b[0;36mChroma.__init__\u001b[0;34m(self, collection_name, embedding_function, persist_directory, client_settings, collection_metadata, client, relevance_score_fn)\u001b[0m\n\u001b[1;32m    113\u001b[0m         _client_settings \u001b[38;5;241m=\u001b[39m chromadb\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mSettings(\n\u001b[1;32m    114\u001b[0m             chroma_db_impl\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mduckdb+parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    115\u001b[0m         )\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m         _client_settings \u001b[38;5;241m=\u001b[39m \u001b[43mchromadb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241m.\u001b[39mSettings(is_persistent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    118\u001b[0m     _client_settings\u001b[38;5;241m.\u001b[39mpersist_directory \u001b[38;5;241m=\u001b[39m persist_directory\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'chromadb' has no attribute 'config'"
     ]
    }
   ],
   "source": [
    "## Embedding the chunks and Vector Store\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# persist_directory = 'chroma-db'\n",
    "# vectordb = Chroma.from_documents(documents, embeddings)\n",
    "# Save the vector store\n",
    "# vectordb.persist()\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=documents,\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not neccesary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'chromadb' has no attribute 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## Embedding the chunks and Vector Store\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Chroma\n\u001b[0;32m----> 4\u001b[0m db \u001b[38;5;241m=\u001b[39m \u001b[43mChroma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain_community/vectorstores/chroma.py:878\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[0;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    876\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    877\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m--> 878\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain_community/vectorstores/chroma.py:814\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28mcls\u001b[39m: Type[Chroma],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    794\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Chroma:\n\u001b[1;32m    795\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a Chroma vectorstore from a raw documents.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m \n\u001b[1;32m    797\u001b[0m \u001b[38;5;124;03m    If a persist_directory is specified, the collection will be persisted there.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;124;03m        Chroma: Chroma vectorstore.\u001b[39;00m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 814\u001b[0m     chroma_collection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    817\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    824\u001b[0m         ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(uuid\u001b[38;5;241m.\u001b[39muuid4()) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m texts]\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:205\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     emit_warning()\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Meet_Patel/new1/newVenv/lib/python3.12/site-packages/langchain_community/vectorstores/chroma.py:120\u001b[0m, in \u001b[0;36mChroma.__init__\u001b[0;34m(self, collection_name, embedding_function, persist_directory, client_settings, collection_metadata, client, relevance_score_fn)\u001b[0m\n\u001b[1;32m    118\u001b[0m     _client_settings\u001b[38;5;241m.\u001b[39mpersist_directory \u001b[38;5;241m=\u001b[39m persist_directory\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     _client_settings \u001b[38;5;241m=\u001b[39m \u001b[43mchromadb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241m.\u001b[39mSettings()\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_settings \u001b[38;5;241m=\u001b[39m _client_settings\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client \u001b[38;5;241m=\u001b[39m chromadb\u001b[38;5;241m.\u001b[39mClient(_client_settings)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'chromadb' has no attribute 'config'"
     ]
    }
   ],
   "source": [
    "## Embedding the chunks and Vector Store\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "db = Chroma.from_documents(documents, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## vector database\n",
    "query = \"Who are the others of FUSECAP: Leveraging Large Language Models for Enriched Fused Image Captions reserch paper?\"\n",
    "result = db.similarity_search(query)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FAISS VECTOR DATABASE\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"llama3\")\n",
    "db2 = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "# Initialize your FAISS index\n",
    "index = db2.index  # Access the underlying FAISS index\n",
    "\n",
    "# Define the path to the file where you want to save the index\n",
    "index_file_path = 'faiss_index.bin'\n",
    "\n",
    "# Save the index to a file\n",
    "faiss.write_index(index, index_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the docstore to a file\n",
    "with open('docstore.pkl', 'wb') as f:\n",
    "    pickle.dump(db2.docstore, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "# Define the path to the file where the index is saved\n",
    "index_file_path = 'faiss_index.bin'\n",
    "\n",
    "# Load the index from the file\n",
    "index = faiss.read_index(index_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the docstore from the file\n",
    "with open('docstore.pkl', 'rb') as f:\n",
    "    docstore = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Create a new FAISS vectorstore with the loaded index and docstore\n",
    "db3 = FAISS(\n",
    "    embedding_function=embeddings,  # Make sure to initialize embeddings as needed\n",
    "    index=index,\n",
    "    docstore=docstore,\n",
    "    index_to_docstore_id=db2.index_to_docstore_id,  # or recreate if needed\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'passing it.\\nA.2. Training Set Generation for LLM Fuser with\\nChatGPT.\\nAs outlined in Section 3.2, we harness the zero-shot ca-\\npabilities of ChatGPT to generate a compact dataset encom-\\npassing 20,000 examples. This data is subsequently used\\nto fine-tune an open-source Large Language Model (LLM).\\nThe prompt provided to ChatGPT is as follows:\\n”A caption of an image is given: original caption .\\nThe following objects are detected in the image from left to\\nright:\\nAa1\\n1, ...,ak−1\\n1andak\\n1o1[with the following text: t1].\\n...\\nAa1\\nn, ...,akn−1\\nn andaknnon[with the following text: tn].\\nWrite a comprehensive and concise caption of the scene\\nusing the objects detected.”'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"How many components are avilable in the architecture of the FUSE cap model?\"\n",
    "result = db3.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ollama(model='llama3')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama3\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Designing the Chatprompt Template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\"\n",
    "Answer the following question based only on the provided context\n",
    "Think step by step before providing a detailed answer.\n",
    "I will tip you $1000 if the used fiinds the answer helpful.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Question: {input}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Stuff Document Chain\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7f7720e37e00>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = db3.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retriever Chain \n",
    "from langchain.chains import create_retrieval_chain\n",
    "retrival_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = retrival_chain.invoke({\"input\":\"What is LLM Fuser?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, I can answer your question.\\n\\nLLM Fuser refers to a model that combines (fuses) enriched captions generated by ChatGPT with an open-source Large Language Model (LLM). The specific prompt used for generating these captions involves providing an image and detected objects in the image, along with their corresponding text descriptions.'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
